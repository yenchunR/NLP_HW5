{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMnZ0kY884WpZfzyfQuq6Dz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yenchunR/NLP_HW5/blob/master/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvN4PcVOOwzf",
        "outputId": "9e9e4667-7aac-42e7-fa10-4398054c80ac"
      },
      "source": [
        "!pip3 install opencc-python-reimplemented"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencc-python-reimplemented\n",
            "  Downloading opencc-python-reimplemented-0.1.6.tar.gz (484 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 33.1 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 61 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 92 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 102 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 112 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 122 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 133 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 143 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 153 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 163 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 174 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 184 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 194 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 204 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 215 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 225 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 235 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 245 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 256 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 266 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 276 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 286 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 296 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 307 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 317 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 327 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 337 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 348 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 358 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 368 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 378 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 389 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 399 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 409 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 419 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 430 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 440 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 450 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 460 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 471 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 481 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 484 kB 7.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: opencc-python-reimplemented\n",
            "  Building wheel for opencc-python-reimplemented (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opencc-python-reimplemented: filename=opencc_python_reimplemented-0.1.6-py2.py3-none-any.whl size=486150 sha256=ebb4876a42fd7ff42938bceb810196112443af2b40099151ac0b5642cfa639d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/e2/60/d062d260be08788bb389521544a8fc173de9a9a78d6a593344\n",
            "Successfully built opencc-python-reimplemented\n",
            "Installing collected packages: opencc-python-reimplemented\n",
            "Successfully installed opencc-python-reimplemented-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSOa1hBoXwvh",
        "outputId": "2aa54664-b3a9-403a-de5b-f08466557df5"
      },
      "source": [
        "!pip3 install zhon"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting zhon\n",
            "  Downloading zhon-1.1.5.tar.gz (99 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▎                            | 10 kB 26.6 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 20 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 30 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 40 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 51 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 71 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 92 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 99 kB 5.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: zhon\n",
            "  Building wheel for zhon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zhon: filename=zhon-1.1.5-py3-none-any.whl size=84322 sha256=653c45e2c1a0afd9ea5ede74175345dde3306eb91a6b0131d524a20891cb4b16\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/56/17/2675c4c7413a72bf173062e8d0a16503e3b2607745aa84988d\n",
            "Successfully built zhon\n",
            "Installing collected packages: zhon\n",
            "Successfully installed zhon-1.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jLTX9zBwHlZ",
        "outputId": "b31aea34-006f-4f97-a7d7-c7b6b1e9bb34"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN8-WTSxRZPg"
      },
      "source": [
        "#數據處理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNuvbNTvRbGS"
      },
      "source": [
        "## 分割英文數據和中文數據"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVD-05yx4xzT",
        "outputId": "5e5be8fc-4e35-4686-b4fe-bc54a48ac952"
      },
      "source": [
        "# coding=utf-8\n",
        "from opencc import OpenCC\n",
        "import json\n",
        "import re\n",
        "\n",
        "trainPath = '/content/drive/My Drive/translation2019zh/translation2019zh_train.json'\n",
        "\n",
        "# Initial\n",
        "cc = OpenCC('s2t')\n",
        "\n",
        "en_data = []\n",
        "ch_data = []\n",
        "index = 0\n",
        "\n",
        "with open(trainPath, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        data = json.loads(line)\n",
        "        en_data.append(data['english'])\n",
        "        ch_data.append('\\t'+ cc.convert(data['chinese']) + '\\n')\n",
        "        if index > 50:\n",
        "            break\n",
        "        index += 1\n",
        "print(en_data[:10])\n",
        "print(ch_data[:10])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['For greater sharpness, but with a slight increase in graininess, you can use a 1:1 dilution of this developer.', 'He calls the Green Book, his book of teachings, “the new gospel.', 'And the light breeze moves me to caress her long ear', 'They have the blood of martyrs is the White to flow …', \"Finally, the Lakers head to the Motor City to take on a Pistons team that currently owns the Eastern Conference's second best record (1/31). L.\", '\"The perfect match—my father loves names and Jackie loves money, \" sneered Alexander at the wedding. Neither he nor Christina ever got along with their stepmother17.', 'In 2006, Walmart was charged with racism when its recommendation engine paired Planet of the Apes with a documentary about Martin Luther King.', 'The matte as main copper phase in the cleaning. slag was deter- mined by electron probe microscopic analysis.', 'Have you shined your shoes?', 'The Tanning Matrix can be formed by resorcinol and oxazolidine E, and the reactioncharateristics between Tanning Matrix and collagen were investigated through NMR and size distribution analysis.']\n",
            "['\\t爲了更好的銳度，但是附帶的會多一些顆粒度，可以使用這個顯影劑的1：1稀釋液。\\n', '\\t他還把宣揚自己思想的所謂《綠皮書》稱作“新福音書”。\\n', '\\t微風推着我去愛撫它的長耳朵\\n', '\\t它們的先烈們的鮮血是白流了…\\n', '\\t最後，在1月31日，湖人將前往汽車城底特律挑戰活塞隊，活塞近來在東部排名第二。\\n', '\\t“真是天造地設的一對——我父親喜歡結交名人，傑姬酷愛金錢，”亞歷山大在婚禮上譏諷道。他和克里斯蒂娜從未同他們的繼母和睦相處過。\\n', '\\t2006年，沃爾瑪的推薦引擎竟將《人猿星球》與馬丁·路德·金的記錄片配成了一對，爲此沃爾瑪遭到了種族歧視的指控。\\n', '\\t通過電子探針顯微分析確定貧化渣中主要銅相爲冰銅相。\\n', '\\t吉姆靠給人擦皮鞋爲生。\\n', '\\t用甘氨酸模擬膠原，研究間苯二酚-惡唑烷E鞣性基質的形成以及與膠原之間的反應特性。\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfaobcGgRhq5"
      },
      "source": [
        "## 分別生成中英文字典"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zT0ojIMOOv6",
        "outputId": "e8c5091c-843b-40c0-c306-0a8bf724d65d"
      },
      "source": [
        "import string\n",
        "from zhon.hanzi import punctuation\n",
        "\n",
        "en_vocab = set(''.join(en_data))\n",
        "id2en = list(en_vocab)\n",
        "en2id = {c:i for i,c in enumerate(id2en)}\n",
        "\n",
        "ch_vocab = set(''.join(ch_data))\n",
        "id2ch = list(ch_vocab)\n",
        "ch2id = {c:i for i,c in enumerate(id2ch)}\n",
        "\n",
        "print('\\n英文字典:\\n', en2id)\n",
        "print('\\n中文字典共計\\n:', ch2id)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "英文字典:\n",
            " ['I', 'S', 'j', 'w', ')', '9', 'i', ':', 'k', '/', 'q', 'r', 'U', 'v', 'D', 'x', 'h', 'P', 'O', '8', 'R', 'p', 'W', 'E', 'f', '3', 'A', 'C', '.', ',', 'l', '2', 'a', '“', 'N', '&', '…', \"'\", '5', '0', 't', 'L', '—', 'G', 'o', 'u', 'c', 'F', '1', 'g', 'b', 'B', 'z', 'T', '6', 'Y', 'y', 's', 'J', 'M', '7', '-', '?', 'm', ' ', 'H', 'e', 'n', '\"', 'd', 'K', '(']\n",
            "\n",
            "中文字典共計\n",
            ": {'I': 0, '覽': 1, '求': 2, '鋼': 3, '須': 4, '運': 5, '功': 6, '物': 7, '邊': 8, '快': 9, '襪': 10, '行': 11, '必': 12, '律': 13, '碼': 14, '艾': 15, '內': 16, '腳': 17, '拿': 18, '博': 19, '粒': 20, '買': 21, '狗': 22, '杯': 23, '賺': 24, '表': 25, '…': 26, '合': 27, '酸': 28, '迷': 29, '比': 30, '似': 31, '音': 32, '水': 33, '加': 34, '協': 35, '貧': 36, '究': 37, '份': 38, '棄': 39, '出': 40, '探': 41, '至': 42, '失': 43, '麗': 44, '段': 45, '（': 46, '昨': 47, '見': 48, '應': 49, '麥': 50, '羣': 51, '爲': 52, '習': 53, '己': 54, '鞣': 55, '瀏': 56, '喜': 57, '諷': 58, 'E': 59, '、': 60, '幾': 61, '亟': 62, '願': 63, '銘': 64, '將': 65, '載': 66, '增': 67, '正': 68, '：': 69, '使': 70, '患': 71, '檸': 72, '底': 73, '相': 74, '極': 75, '跳': 76, '損': 77, '散': 78, '撫': 79, '爾': 80, '無': 81, '員': 82, '環': 83, '8': 84, '美': 85, '挑': 86, '兒': 87, '院': 88, '之': 89, '點': 90, '特': 91, '家': 92, 'm': 93, '退': 94, '那': 95, '。': 96, '承': 97, '所': 98, '再': 99, '種': 100, '造': 101, '歌': 102, '監': 103, '起': 104, '第': 105, '授': 106, '研': 107, '李': 108, '太': 109, '並': 110, '赤': 111, '寄': 112, '管': 113, '巴': 114, '續': 115, '定': 116, '備': 117, '宣': 118, '過': 119, '往': 120, '共': 121, '源': 122, '製': 123, '擊': 124, '立': 125, '影': 126, '聽': 127, '唑': 128, '法': 129, '新': 130, '山': 131, '抗': 132, '思': 133, '6': 134, '什': 135, '校': 136, '涉': 137, '與': 138, '）': 139, '來': 140, '破': 141, '歷': 142, '社': 143, '》': 144, '由': 145, '去': 146, '着': 147, '展': 148, '結': 149, 'K': 150, '認': 151, '障': 152, '\\t': 153, '苓': 154, '愛': 155, '國': 156, '視': 157, '明': 158, '月': 159, '育': 160, '爪': 161, '以': 162, '題': 163, '的': 164, '此': 165, '等': 166, '地': 167, '單': 168, '越': 169, '予': 170, '規': 171, '即': 172, '場': 173, '當': 174, '生': 175, '程': 176, '關': 177, '勒': 178, '市': 179, '維': 180, 'u': 181, '隊': 182, '戰': 183, '亞': 184, '檬': 185, '慮': 186, '確': 187, '會': 188, '機': 189, '資': 190, '推': 191, '宮': 192, '試': 193, '稀': 194, '前': 195, '睦': 196, '揭': 197, '品': 198, '塊': 199, '文': 200, '些': 201, '帖': 202, '隨': 203, '權': 204, '同': 205, '「': 206, '猿': 207, '拉': 208, '凍': 209, '羅': 210, '期': 211, '板': 212, '羞': 213, '自': 214, '是': 215, '2': 216, '位': 217, '娜': 218, '最': 219, '教': 220, '態': 221, '適': 222, '彎': 223, '實': 224, '道': 225, '器': 226, '交': 227, '指': 228, '-': 229, '而': 230, '鮮': 231, '鎖': 232, '濟': 233, '版': 234, '界': 235, '析': 236, '苯': 237, '很': 238, '亦': 239, '涵': 240, '義': 241, '藥': 242, '錨': 243, '奮': 244, '伊': 245, '俱': 246, '說': 247, '澡': 248, '突': 249, '傷': 250, '；': 251, '常': 252, '還': 253, '化': 254, '裝': 255, '身': 256, '重': 257, '近': 258, '聯': 259, '益': 260, '人': 261, '解': 262, '爭': 263, '均': 264, '火': 265, '現': 266, '基': 267, '及': 268, '高': 269, '不': 270, '穩': 271, '存': 272, '算': 273, '庫': 274, '寶': 275, '胞': 276, '，': 277, '小': 278, '贏': 279, '作': 280, '完': 281, '穿': 282, '根': 283, '姆': 284, '秀': 285, '日': 286, '者': 287, '齡': 288, '盟': 289, '貨': 290, '烷': 291, '給': 292, '婦': 293, '留': 294, '顆': 295, '帶': 296, '血': 297, '瑪': 298, '熱': 299, '毛': 300, '達': 301, '」': 302, '中': 303, '足': 304, '蘭': 305, '章': 306, '微': 307, '聲': 308, '考': 309, '塞': 310, '片': 311, '世': 312, '限': 313, '代': 314, '0': 315, 'L': 316, '盾': 317, '逐': 318, '城': 319, '急': 320, '弗': 321, '莉': 322, '茯': 323, '?': 324, '族': 325, '韓': 326, '鞋': 327, '冰': 328, ' ': 329, '瞭': 330, '繼': 331, '責': 332, '專': 333, '就': 334, '痛': 335, '銳': 336, '都': 337, '格': 338, '嚴': 339, '一': 340, '烈': 341, '站': 342, '亮': 343, '籃': 344, '活': 345, '稱': 346, '記': 347, '興': 348, '下': 349, 'A': 350, '.': 351, '心': 352, '性': 353, '編': 354, '容': 355, '遠': 356, 'a': 357, '許': 358, '成': 359, '辦': 360, '用': 361, '打': 362, '控': 363, '尖': 364, '獨': 365, '階': 366, '費': 367, '參': 368, '母': 369, '汽': 370, '東': 371, '業': 372, '石': 373, '二': 374, '別': 375, '產': 376, '改': 377, '病': 378, '做': 379, '酚': 380, '朵': 381, '工': 382, '我': 383, '耳': 384, '紅': 385, '揚': 386, '準': 387, '車': 388, '整': 389, '混': 390, '疑': 391, '取': 392, '絲': 393, '擬': 394, '獄': 395, '且': 396, '雲': 397, '需': 398, '子': 399, '冬': 400, '動': 401, '南': 402, '皮': 403, '禮': 404, '在': 405, '華': 406, '語': 407, '堂': 408, '升': 409, '服': 410, '質': 411, '案': 412, '各': 413, '天': 414, '纔': 415, '1': 416, '主': 417, '度': 418, '查': 419, '公': 420, '著': 421, '譏': 422, '哈': 423, '優': 424, '統': 425, '則': 426, '附': 427, '大': 428, '情': 429, '垂': 430, '識': 431, '剪': 432, '知': 433, '液': 434, '長': 435, '貴': 436, '上': 437, '組': 438, '類': 439, '測': 440, '斯': 441, '送': 442, '架': 443, '渣': 444, '時': 445, '部': 446, '更': 447, '價': 448, '個': 449, '席': 450, '久': 451, '擴': 452, '緩': 453, '柔': 454, '步': 455, '檢': 456, '原': 457, '裏': 458, '遭': 459, '球': 460, '譯': 461, '神': 462, '本': 463, '滋': 464, '傢': 465, '丹': 466, '報': 467, '未': 468, '先': 469, '金': 470, '任': 471, '衰': 472, '面': 473, '流': 474, '排': 475, '誰': 476, '夜': 477, '遊': 478, '放': 479, '調': 480, '演': 481, '經': 482, '綠': 483, '停': 484, '示': 485, '然': 486, '郵': 487, '靠': 488, '孫': 489, '鼓': 490, '親': 491, '尼': 492, '俄': 493, '跨': 494, '姬': 495, '浴': 496, '漫': 497, '濃': 498, '軀': 499, '擔': 500, '候': 501, '設': 502, '已': 503, '州': 504, '細': 505, '學': 506, '樣': 507, '癌': 508, '注': 509, '歧': 510, '保': 511, '旱': 512, '目': 513, '汰': 514, '於': 515, '開': 516, '志': 517, '首': 518, '供': 519, '嗎': 520, '錄': 521, '標': 522, '電': 523, '麼': 524, '響': 525, '路': 526, '如': 527, '避': 528, '遺': 529, '惡': 530, '它': 531, '得': 532, '盛': 533, '安': 534, '戶': 535, '況': 536, '書': 537, '毒': 538, '錢': 539, '免': 540, '阿': 541, '言': 542, '濁': 543, '汁': 544, '空': 545, '吉': 546, '刮': 547, '\\n': 548, '波': 549, '噸': 550, '映': 551, '年': 552, '疼': 553, '顯': 554, '湯': 555, '擎': 556, '告': 557, '頓': 558, '變': 559, '溫': 560, '氨': 561, '核': 562, '敢': 563, '反': 564, '風': 565, '湖': 566, '分': 567, '擦': 568, '沃': 569, '有': 570, '酷': 571, '能': 572, '薦': 573, '白': 574, '感': 575, '負': 576, '但': 577, '被': 578, '夫': 579, 'O': 580, '寒': 581, '克': 582, '古': 583, '傳': 584, '園': 585, 'l': 586, '慢': 587, '伴': 588, '其': 589, '藏': 590, '屬': 591, '—': 592, '決': 593, '體': 594, '銷': 595, '和': 596, '星': 597, '蒂': 598, '海': 599, '形': 600, '始': 601, '們': 602, '可': 603, '把': 604, '父': 605, '福': 606, '對': 607, '冷': 608, '住': 609, '咱': 610, '因': 611, '矛': 612, '鄧': 613, '型': 614, '花': 615, '里': 616, '沒': 617, '婚': 618, '舒': 619, '回': 620, '到': 621, '曉': 622, '他': 623, '據': 624, '享': 625, '女': 626, '今': 627, '三': 628, '印': 629, '間': 630, '強': 631, '待': 632, '純': 633, '丁': 634, '墊': 635, '境': 636, '包': 637, '數': 638, '配': 639, '腹': 640, '理': 641, '十': 642, '模': 643, '集': 644, '暖': 645, '要': 646, '軟': 647, '？': 648, '平': 649, '式': 650, '級': 651, '永': 652, '9': 653, '劑': 654, '杆': 655, '提': 656, '持': 657, '非': 658, '溜': 659, '方': 660, '頸': 661, '息': 662, '檔': 663, '傑': 664, '竟': 665, '難': 666, '真': 667, '處': 668, '意': 669, '張': 670, '想': 671, '好': 672, '符': 673, '“': 674, '《': 675, '通': 676, '5': 677, '果': 678, '問': 679, '你': 680, '預': 681, '箱': 682, '發': 683, '進': 684, '才': 685, '後': 686, '力': 687, '晚': 688, '陪': 689, '兩': 690, '妻': 691, '精': 692, '士': 693, '”': 694, '網': 695, '歲': 696, '甘': 697, '了': 698, '洗': 699, '銀': 700, '字': 701, '\"': 702, '莎': 703, '信': 704, '答': 705, '多': 706, '令': 707, '勝': 708, '量': 709, '慰': 710, '淘': 711, '寵': 712, '名': 713, '復': 714, '謝': 715, '這': 716, '謂': 717, '曲': 718, '利': 719, '3': 720, '飾': 721, '從': 722, 'C': 723, '讓': 724, '劇': 725, '銅': 726, '稿': 727, '·': 728, '節': 729, '布': 730, '馬': 731, '德': 732, '飛': 733, '膠': 734, '客': 735, '直': 736, '針': 737, '歡': 738, '隱': 739, '降': 740, '富': 741, '或': 742, '區': 743, '證': 744, '看': 745, '引': 746, '際': 747, '外': 748, '獲': 749, '男': 750, '釋': 751}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5eC76yGRpfX"
      },
      "source": [
        "## 轉換輸入數據格式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdnWPNeCRqvt",
        "outputId": "abc34049-aa73-4b68-c194-d3eee7ee336e"
      },
      "source": [
        "# 利用字典，映射數據\n",
        "en_num_data = [[en2id[en] for en in line ] for line in en_data]\n",
        "ch_num_data = [[ch2id[ch] for ch in line] for line in ch_data]\n",
        "de_num_data = [[ch2id[ch] for ch in line][1:] for line in ch_data]\n",
        "\n",
        "print('char:', en_data[1])\n",
        "print('index:', en_num_data[1])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "char: He calls the Green Book, his book of teachings, “the new gospel.\n",
            "index: [65, 66, 64, 46, 32, 30, 30, 57, 64, 40, 16, 66, 64, 43, 11, 66, 66, 67, 64, 51, 44, 44, 8, 29, 64, 16, 6, 57, 64, 50, 44, 44, 8, 64, 44, 24, 64, 40, 66, 32, 46, 16, 6, 67, 49, 57, 29, 64, 33, 40, 16, 66, 64, 67, 66, 3, 64, 49, 44, 57, 21, 66, 30, 28]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV12rmpwRyTk"
      },
      "source": [
        "## 將訓練數據進行onehot編碼"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEQ6svRMR3xs",
        "outputId": "4794e651-e120-435d-88da-bb62fd270da8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 獲取輸入輸出端的最大長度\n",
        "max_encoder_seq_length = max([len(txt) for txt in en_num_data])\n",
        "max_decoder_seq_length = max([len(txt) for txt in ch_num_data])\n",
        "print('max encoder length:', max_encoder_seq_length)\n",
        "print('max decoder length:', max_decoder_seq_length)\n",
        "\n",
        "# 將數據進行onehot處理\n",
        "encoder_input_data = np.zeros((len(en_num_data), max_encoder_seq_length, len(en2id)), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
        "\n",
        "for i in range(len(ch_num_data)):\n",
        "    for t, j in enumerate(en_num_data[i]):\n",
        "        encoder_input_data[i, t, j] = 1.\n",
        "    for t, j in enumerate(ch_num_data[i]):\n",
        "        decoder_input_data[i, t, j] = 1.\n",
        "    for t, j in enumerate(de_num_data[i]):\n",
        "        decoder_target_data[i, t, j] = 1.\n",
        "\n",
        "print('index data:\\n', en_num_data[1])\n",
        "print('one hot data:\\n', encoder_input_data[1])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max encoder length: 197\n",
            "max decoder length: 70\n",
            "index data:\n",
            " [65, 66, 64, 46, 32, 30, 30, 57, 64, 40, 16, 66, 64, 43, 11, 66, 66, 67, 64, 51, 44, 44, 8, 29, 64, 16, 6, 57, 64, 50, 44, 44, 8, 64, 44, 24, 64, 40, 66, 32, 46, 16, 6, 67, 49, 57, 29, 64, 33, 40, 16, 66, 64, 67, 66, 3, 64, 49, 44, 57, 21, 66, 30, 28]\n",
            "one hot data:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCuqO5jZR-Ja"
      },
      "source": [
        "# 模型選擇與建模"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gysWv2KPSCmk"
      },
      "source": [
        "## 參數設置"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSb_xY_jSF2W"
      },
      "source": [
        "# =======預定義模型參數========\n",
        "EN_VOCAB_SIZE = len(en2id)\n",
        "CH_VOCAB_SIZE = len(ch2id)\n",
        "HIDDEN_SIZE = 256\n",
        "\n",
        "LEARNING_RATE = 0.003\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 200"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA8NyDlKSJvJ"
      },
      "source": [
        "## encoder建模"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1RSmtCtSB-O"
      },
      "source": [
        "# ======================================keras model==================================\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "# ==============encoder=============\n",
        "encoder_inputs = Input(shape=(None, EN_VOCAB_SIZE))\n",
        "#emb_inp = Embedding(output_dim=HIDDEN_SIZE, input_dim=EN_VOCAB_SIZE)(encoder_inputs)\n",
        "encoder_h1, encoder_state_h1, encoder_state_c1 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)(encoder_inputs)\n",
        "encoder_h2, encoder_state_h2, encoder_state_c2 = LSTM(HIDDEN_SIZE, return_state=True)(encoder_h1)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfANihbBST3N"
      },
      "source": [
        "## decoder建模"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUbaIladSXM-"
      },
      "source": [
        "# ==============decoder=============\n",
        "decoder_inputs = Input(shape=(None, CH_VOCAB_SIZE))\n",
        "\n",
        "#emb_target = Embedding(output_dim=HIDDEN_SIZE, input_dim=CH_VOCAB_SIZE, mask_zero=True)(decoder_inputs)\n",
        "lstm1 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)\n",
        "lstm2 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)\n",
        "decoder_dense = Dense(CH_VOCAB_SIZE, activation='softmax')\n",
        "\n",
        "decoder_h1, _, _ = lstm1(decoder_inputs, initial_state=[encoder_state_h1, encoder_state_c1])\n",
        "decoder_h2, _, _ = lstm2(decoder_h1, initial_state=[encoder_state_h2, encoder_state_c2])\n",
        "decoder_outputs = decoder_dense(decoder_h2)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlfbOG9jSiEP"
      },
      "source": [
        "## 訓練模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeBC_TcYSjHJ",
        "outputId": "17a62b76-26d4-4114-e9f9-4f67c73858f6"
      },
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "opt = Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          validation_split=0.)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, None, 72)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           [(None, None, 752)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   [(None, None, 256),  336896      input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  [(None, None, 256),  1033216     input_14[0][0]                   \n",
            "                                                                 lstm_8[0][1]                     \n",
            "                                                                 lstm_8[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   [(None, 256), (None, 525312      lstm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  [(None, None, 256),  525312      lstm_10[0][0]                    \n",
            "                                                                 lstm_9[0][1]                     \n",
            "                                                                 lstm_9[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, None, 752)    193264      lstm_11[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,614,000\n",
            "Trainable params: 2,614,000\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 3.3351 - accuracy: 2.7473e-04\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 3.3284 - accuracy: 0.0286\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 3.3025 - accuracy: 0.0288\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 3.1308 - accuracy: 0.0159\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 3.0916 - accuracy: 0.0184\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 3.0714 - accuracy: 0.0203\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 3.0607 - accuracy: 0.0234\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 3.0515 - accuracy: 0.0220\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 3.0449 - accuracy: 0.0217\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 3.0348 - accuracy: 0.0225\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 3.0280 - accuracy: 0.0225\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 3.0180 - accuracy: 0.0223\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 3.0185 - accuracy: 0.0206\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 3.0112 - accuracy: 0.0206\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 3.0025 - accuracy: 0.0212\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.9930 - accuracy: 0.0220\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 2.9847 - accuracy: 0.0220\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.9767 - accuracy: 0.0209\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.9704 - accuracy: 0.0209\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.9655 - accuracy: 0.0212\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.9620 - accuracy: 0.0214\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.9572 - accuracy: 0.0212\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.9527 - accuracy: 0.0217\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.9484 - accuracy: 0.0228\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.9441 - accuracy: 0.0231\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.9398 - accuracy: 0.0225\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.9426 - accuracy: 0.0225\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.9327 - accuracy: 0.0236\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.9301 - accuracy: 0.0231\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 2.9272 - accuracy: 0.0231\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.9223 - accuracy: 0.0236\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.9261 - accuracy: 0.0228\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.9153 - accuracy: 0.0239\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.9122 - accuracy: 0.0236\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.9115 - accuracy: 0.0234\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.9071 - accuracy: 0.0239\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.9179 - accuracy: 0.0228\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.9069 - accuracy: 0.0225\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.9038 - accuracy: 0.0236\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.9038 - accuracy: 0.0225\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.9028 - accuracy: 0.0225\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.8916 - accuracy: 0.0242\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.8879 - accuracy: 0.0242\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.8909 - accuracy: 0.0228\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.8856 - accuracy: 0.0231\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.8830 - accuracy: 0.0239\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.8818 - accuracy: 0.0234\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.8783 - accuracy: 0.0234\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.8760 - accuracy: 0.0245\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.8771 - accuracy: 0.0234\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.8701 - accuracy: 0.0239\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.8704 - accuracy: 0.0239\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.8707 - accuracy: 0.0236\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.8680 - accuracy: 0.0236\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.8621 - accuracy: 0.0242\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.8576 - accuracy: 0.0236\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.8592 - accuracy: 0.0242\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 2.8535 - accuracy: 0.0242\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.8520 - accuracy: 0.0247\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.8443 - accuracy: 0.0245\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.8473 - accuracy: 0.0242\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.8393 - accuracy: 0.0245\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.8382 - accuracy: 0.0242\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.8326 - accuracy: 0.0247\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.8305 - accuracy: 0.0250\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.8273 - accuracy: 0.0247\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 2.8260 - accuracy: 0.0247\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.8217 - accuracy: 0.0250\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.8176 - accuracy: 0.0261\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.8138 - accuracy: 0.0258\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.8128 - accuracy: 0.0247\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.8167 - accuracy: 0.0253\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 2.8408 - accuracy: 0.0231\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.8121 - accuracy: 0.0250\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.8308 - accuracy: 0.0247\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.8097 - accuracy: 0.0250\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.8218 - accuracy: 0.0250\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.8018 - accuracy: 0.0247\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.8096 - accuracy: 0.0250\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.7931 - accuracy: 0.0253\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 2.8025 - accuracy: 0.0247\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 2.7917 - accuracy: 0.0250\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.7931 - accuracy: 0.0255\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 2.7848 - accuracy: 0.0258\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.7872 - accuracy: 0.0255\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.7861 - accuracy: 0.0258\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.7869 - accuracy: 0.0253\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.7966 - accuracy: 0.0253\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.7937 - accuracy: 0.0250\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.8015 - accuracy: 0.0253\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.7836 - accuracy: 0.0258\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.7754 - accuracy: 0.0250\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.7738 - accuracy: 0.0250\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.7662 - accuracy: 0.0253\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.7682 - accuracy: 0.0253\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.7809 - accuracy: 0.0245\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.7685 - accuracy: 0.0247\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.7869 - accuracy: 0.0245\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.7820 - accuracy: 0.0250\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.7714 - accuracy: 0.0250\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 2.7692 - accuracy: 0.0250\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 2.7684 - accuracy: 0.0242\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.7680 - accuracy: 0.0242\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.7569 - accuracy: 0.0253\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.7655 - accuracy: 0.0245\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.7701 - accuracy: 0.0253\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.7703 - accuracy: 0.0253\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.7512 - accuracy: 0.0253\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.7598 - accuracy: 0.0253\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.7426 - accuracy: 0.0258\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.7424 - accuracy: 0.0253\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.7329 - accuracy: 0.0258\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.7316 - accuracy: 0.0266\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.7293 - accuracy: 0.0264\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.7591 - accuracy: 0.0261\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.7417 - accuracy: 0.0264\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.7350 - accuracy: 0.0272\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.7339 - accuracy: 0.0269\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.7259 - accuracy: 0.0275\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.7215 - accuracy: 0.0266\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.7142 - accuracy: 0.0258\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.7105 - accuracy: 0.0255\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.7014 - accuracy: 0.0258\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.6932 - accuracy: 0.0272\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.6909 - accuracy: 0.0272\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.6862 - accuracy: 0.0277\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.6798 - accuracy: 0.0266\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.6758 - accuracy: 0.0286\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.6715 - accuracy: 0.0291\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 2.6680 - accuracy: 0.0288\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.6631 - accuracy: 0.0294\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.6677 - accuracy: 0.0291\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.6637 - accuracy: 0.0286\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.6595 - accuracy: 0.0288\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.6525 - accuracy: 0.0297\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 2.6481 - accuracy: 0.0288\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.6447 - accuracy: 0.0294\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.6395 - accuracy: 0.0308\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.6330 - accuracy: 0.0297\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.6286 - accuracy: 0.0308\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.6241 - accuracy: 0.0310\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 2.6178 - accuracy: 0.0294\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.6140 - accuracy: 0.0299\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.6109 - accuracy: 0.0291\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.6104 - accuracy: 0.0294\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.6102 - accuracy: 0.0291\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.6063 - accuracy: 0.0299\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 2.5970 - accuracy: 0.0294\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.5934 - accuracy: 0.0291\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.5878 - accuracy: 0.0297\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 2.5830 - accuracy: 0.0288\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 2.5997 - accuracy: 0.0308\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.5988 - accuracy: 0.0294\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.5955 - accuracy: 0.0291\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.5876 - accuracy: 0.0294\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.5828 - accuracy: 0.0299\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.5994 - accuracy: 0.0299\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.5993 - accuracy: 0.0272\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.6003 - accuracy: 0.0297\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.5921 - accuracy: 0.0286\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.5798 - accuracy: 0.0291\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 2.5799 - accuracy: 0.0302\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.5746 - accuracy: 0.0283\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.5630 - accuracy: 0.0324\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 2.5557 - accuracy: 0.0341\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.5472 - accuracy: 0.0302\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.5401 - accuracy: 0.0316\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.5347 - accuracy: 0.0360\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.5227 - accuracy: 0.0371\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.5182 - accuracy: 0.0363\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.5115 - accuracy: 0.0387\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.5052 - accuracy: 0.0365\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.5094 - accuracy: 0.0352\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.4988 - accuracy: 0.0385\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.4943 - accuracy: 0.0420\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.4818 - accuracy: 0.0412\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.4776 - accuracy: 0.0385\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.4754 - accuracy: 0.0415\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.4622 - accuracy: 0.0448\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.4582 - accuracy: 0.0434\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.4536 - accuracy: 0.0462\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.4445 - accuracy: 0.0481\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.4384 - accuracy: 0.0484\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.4342 - accuracy: 0.0473\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.4252 - accuracy: 0.0508\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.4184 - accuracy: 0.0527\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.4165 - accuracy: 0.0541\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.4066 - accuracy: 0.0544\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.4017 - accuracy: 0.0549\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.3925 - accuracy: 0.0577\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 2.3868 - accuracy: 0.0555\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.3814 - accuracy: 0.0574\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 2.3760 - accuracy: 0.0574\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.3770 - accuracy: 0.0588\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.3707 - accuracy: 0.0560\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.3793 - accuracy: 0.0599\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 2.3668 - accuracy: 0.0599\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.3698 - accuracy: 0.0588\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.3690 - accuracy: 0.0629\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.3630 - accuracy: 0.0640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f276c8a1d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0lhTttBSquO"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21FfP_SKSzS0"
      },
      "source": [
        "## 搭建預測模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdQ7P4N-SuKj"
      },
      "source": [
        "# encoder模型和訓練相同\n",
        "encoder_model = Model(encoder_inputs, [encoder_state_h1, encoder_state_c1, encoder_state_h2, encoder_state_c2])\n",
        "\n",
        "# 預測模型中的decoder的初始化狀態需要傳入新的狀態\n",
        "decoder_state_input_h1 = Input(shape=(HIDDEN_SIZE,))\n",
        "decoder_state_input_c1 = Input(shape=(HIDDEN_SIZE,))\n",
        "decoder_state_input_h2 = Input(shape=(HIDDEN_SIZE,))\n",
        "decoder_state_input_c2 = Input(shape=(HIDDEN_SIZE,))\n",
        "\n",
        "# 使用傳入的值來初始化當前模型的輸入狀態\n",
        "decoder_h1, state_h1, state_c1 = lstm1(decoder_inputs, initial_state=[decoder_state_input_h1, decoder_state_input_c1])\n",
        "decoder_h2, state_h2, state_c2 = lstm2(decoder_h1, initial_state=[decoder_state_input_h2, decoder_state_input_c2])\n",
        "decoder_outputs = decoder_dense(decoder_h2)\n",
        "\n",
        "decoder_model = Model([decoder_inputs, decoder_state_input_h1, decoder_state_input_c1, decoder_state_input_h2, decoder_state_input_c2], \n",
        "                      [decoder_outputs, state_h1, state_c1, state_h2, state_c2])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKYHBCCqS3gF"
      },
      "source": [
        "## 利用預測模型進行翻譯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "294GHUcSS6Ps",
        "outputId": "df278bb3-d811-4e55-98b0-bb08febcbb90"
      },
      "source": [
        "for k in range(3):\n",
        "    test_data = encoder_input_data[k:k+1]\n",
        "    h1, c1, h2, c2 = encoder_model.predict(test_data)\n",
        "    target_seq = np.zeros((1, 1, CH_VOCAB_SIZE))\n",
        "    target_seq[0, 0, ch2id['\\t']] = 1\n",
        "    outputs = []\n",
        "    while True:\n",
        "        output_tokens, h1, c1, h2, c2 = decoder_model.predict([target_seq, h1, c1, h2, c2])\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        outputs.append(sampled_token_index)\n",
        "        target_seq = np.zeros((1, 1, CH_VOCAB_SIZE))\n",
        "        target_seq[0, 0, sampled_token_index] = 1\n",
        "        if sampled_token_index == ch2id['\\n'] or len(outputs) > 20: break\n",
        "    \n",
        "    print(en_data[k])\n",
        "    print(''.join([id2ch[i] for i in outputs]))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For greater sharpness, but with a slight increase in graininess, you can use a 1:1 dilution of this developer.\n",
            "然爾，，的說的的能長長長的的的的的。。。。\n",
            "He calls the Green Book, his book of teachings, “the new gospel.\n",
            "他他他在沒的的的的的的的的，，的的的。。。\n",
            "And the light breeze moves me to caress her long ear\n",
            "然爾，，的說的的能長長長的的的的的。。。。\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}